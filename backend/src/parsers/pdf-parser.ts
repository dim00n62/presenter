// backend/src/parsers/pdf-parser.ts
import { readFile } from 'fs/promises';
import * as pdfjsLib from 'pdfjs-dist/legacy/build/pdf.mjs';
import { semanticChunker, SemanticChunk } from '../services/semantic-chunker.js';

export interface PDFParseResult {
    filename: string;
    text: string;
    metadata: {
        pageCount: number;
        info: any;
    };
    pages: Array<{
        pageNumber: number;
        text: string;
    }>;
}

export class PDFParser {
    async parse(filepath: string): Promise<PDFParseResult> {
        try {
            const buffer = await readFile(filepath);
            const data = new Uint8Array(buffer);

            // Load PDF document
            const loadingTask = pdfjsLib.getDocument({ data });
            const pdfDocument = await loadingTask.promise;

            const numPages = pdfDocument.numPages;
            const pages: Array<{ pageNumber: number; text: string }> = [];
            let fullText = '';

            // Extract text from each page
            for (let i = 1; i <= numPages; i++) {
                const page = await pdfDocument.getPage(i);
                const textContent = await page.getTextContent();

                const pageText = textContent.items
                    .map((item: any) => item.str)
                    .join(' ');

                pages.push({
                    pageNumber: i,
                    text: pageText,
                });

                fullText += pageText + '\n\n';
            }

            // Get metadata
            const metadata = await pdfDocument.getMetadata();

            return {
                filename: filepath,
                text: fullText.trim(),
                metadata: {
                    pageCount: numPages,
                    info: metadata.info,
                },
                pages,
            };
        } catch (error) {
            throw new Error(`PDF parsing failed: ${error.message}`);
        }
    }

    /**
     * üÜï SEMANTIC CHUNKING - —Ä–∞–∑–±–∏–≤–∞–µ—Ç –ø–æ —Å–º—ã—Å–ª—É, –∞ –Ω–µ –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º!
     */
    createTextChunks(result: PDFParseResult): Array<{ content: string; metadata: any }> {
        console.log('üìö [PDF Parser] Creating semantic chunks from full text...');

        // –ò—Å–ø–æ–ª—å–∑—É–µ–º –í–ï–°–¨ —Ç–µ–∫—Å—Ç, –Ω–µ —Ä–∞–∑–±–∏–≤–∞–µ–º –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º
        const fullText = result.text;

        if (!fullText || fullText.trim().length === 0) {
            console.warn('‚ö†Ô∏è No text extracted from PDF - this might be an image-based/scanned PDF');
            console.warn('üí° Try converting to DOCX or use text-based PDF instead');
            return [];
        }

        // Semantic chunking —Å –≥–∏–±—Ä–∏–¥–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–µ–π
        const semanticChunks = semanticChunker.chunk(fullText, {
            strategy: 'hybrid',
            maxChunkSize: 1500,   // ~300 —Å–ª–æ–≤
            minChunkSize: 300,    // ~60 —Å–ª–æ–≤
            overlapSize: 200,     // ~40 —Å–ª–æ–≤ overlap –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            preserveSentences: true,
        });

        // –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è analysis agent
        const chunks = semanticChunks.map((chunk: SemanticChunk) => {
            // –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–∏–º–µ—Ä–Ω—ã–π page number –∏–∑ offset
            const estimatedPage = this.estimatePageNumber(
                chunk.metadata.startOffset,
                result.text.length,
                result.metadata.pageCount
            );

            return {
                content: chunk.content,
                metadata: {
                    type: 'semantic_chunk',
                    chunkIndex: chunk.metadata.chunkIndex,
                    wordCount: chunk.metadata.wordCount,
                    sentences: chunk.metadata.sentences,
                    estimatedPage,  // –ü—Ä–∏–º–µ—Ä–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ (–¥–ª—è —Å–ø—Ä–∞–≤–∫–∏)
                    topics: chunk.metadata.topics,
                    chunkingStrategy: chunk.metadata.type,
                }
            };
        });

        console.log(`‚úÖ [PDF Parser] Created ${chunks.length} semantic chunks (avg ${Math.round(chunks.reduce((sum, c) => sum + c.metadata.wordCount, 0) / chunks.length)} words)`);

        return chunks;
    }

    /**
     * üîß LEGACY: –°—Ç–∞—Ä—ã–π –º–µ—Ç–æ–¥ (–ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º) - –æ—Å—Ç–∞–≤–ª—è–µ–º –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
     */
    createPageBasedChunks(result: PDFParseResult): Array<{ content: string; metadata: any }> {
        console.log('üìÑ [PDF Parser] Creating page-based chunks (legacy mode)...');

        const chunks: Array<{ content: string; metadata: any }> = [];

        result.pages.forEach(page => {
            if (page.text && page.text.trim()) {
                chunks.push({
                    content: page.text.trim(),
                    metadata: {
                        type: 'page',
                        pageNumber: page.pageNumber,
                        wordCount: page.text.split(/\s+/).length
                    }
                });
            }
        });

        if (chunks.length === 0) {
            console.warn('‚ö†Ô∏è No text extracted from PDF');
        }

        return chunks;
    }

    private estimatePageNumber(offset: number, totalLength: number, totalPages: number): number {
        const ratio = offset / totalLength;
        return Math.ceil(ratio * totalPages) || 1;
    }
}

export const pdfParser = new PDFParser();